AWSTemplateFormatVersion: "2010-09-09"
Description: "Deploys the EC2 Autoscaling, LaunchConfig and Instance for Xray"
Parameters:
  PrivateSubnet1Id:
    Type: 'AWS::EC2::Subnet::Id'
  PrivateSubnet2Id:
    Type: 'AWS::EC2::Subnet::Id'
  KeyPairName:
    Type: AWS::EC2::KeyPair::KeyName
  MinScalingNodes:
    Type: Number
  MaxScalingNodes:
    Type: Number
  DeploymentTag:
    Type: String
  ArtifactoryProduct:
    Description: JFrog Artifactory product you want to install into an AMI.
    AllowedValues:
      - JFrog-Artifactory-Pro
      - JFrog-Artifactory-Enterprise
      - JFrog-Container-Registry
    Default: JFrog-Artifactory-Enterprise
    Type: String
  QsS3BucketName:
    Type: String
  QsS3KeyPrefix:
    Type: String
  QsS3Uri:
    Type: String
  DatabaseDriver:
    Type: String
  DatabaseType:
    Type: String
  DatabaseUser:
    Type: String
  DatabasePassword:
    Type: String
    NoEcho: 'true'
  MasterKey:
    Type: String
    NoEcho: 'true'
  SecurityGroups:
    Type: String
  XrayHostProfile:
    Type: String
  XrayHostRole:
    Type: String
  XrayInstanceType:
    Type: String
  JfrogInternalUrl:
    Type: String
  AnsibleVaultPass:
    Description: Ansiblevault Password to secure the artifactory.yml
    Type: String
    NoEcho: 'true'
  Volume:
    Type: String
  XrayDatabaseUser:
    Type: String
  XrayDatabasePassword:
    Type: String
    NoEcho: 'true'
  XrayMasterDatabaseUrl:
    Type: String
  XrayDatabaseUrl:
    Type: String
  XrayFirstNode:
    Description: Runs database scripts if this is the first node
    Type: String
  XrayVersion:
    Type: String
  UserDataDirectory:
    Description: Directory to store Artifactory data. Can be used to store data (via symlink) in detachable volume
    Type: String
    Default: '/xray-user-data'

# To populate additional mappings use the following with the desired --region
# aws --region us-west-2 ec2 describe-images --owners amazon --filters 'Name=name,Values=amzn-ami-hvm-2018.03.0.20181129-x86_64-gp2' 'Name=state,Values=available' --output json | jq -r '.Images | sort_by(.CreationDate) | last(.[]).ImageId'
Mappings:
  AWSAMIRegionMap:
    ap-northeast-1:
      CentOS7HVM: "ami-00a5245b4816c38e6"
    ap-northeast-2:
      CentOS7HVM: "ami-00dc207f8ba6dc919"
    ap-south-1:
      CentOS7HVM: "ami-0ad42f4f66f6c1cc9"
    ap-southeast-1:
      CentOS7HVM: "ami-05b3bcf7f311194b3"
    ap-southeast-2:
      CentOS7HVM: "ami-02fd0b06f06d93dfc"
    ca-central-1:
      CentOS7HVM: "ami-07423fb63ea0a0930"
    eu-central-1:
      CentOS7HVM: "ami-0cfbf4f6db41068ac"
    eu-west-1:
      CentOS7HVM: "ami-08935252a36e25f85"
    sa-east-1:
      CentOS7HVM: "ami-05145e0b28ad8e0b2"
    us-east-1:
      CentOS7HVM: "ami-0affd4508a5d2481b"
    us-east-2:
      CentOS7HVM: "ami-01e36b7901e884a10"
    us-west-1:
      CentOS7HVM: "ami-098f55b4287a885ba"
    us-west-2:
      CentOS7HVM: "ami-0bc06212a56393ee1"

Conditions:
  IsArtifactoryPro: !Equals [!Ref ArtifactoryProduct, 'JFrog-Artifactory-Pro']

Resources:
  XrayScalingGroup:
    Type: AWS::AutoScaling::AutoScalingGroup
    Properties:
      LaunchConfigurationName: !Ref XrayLaunchConfiguration
      VPCZoneIdentifier:
        !If [IsArtifactoryPro, [!Ref PrivateSubnet1Id], [!Ref PrivateSubnet1Id, !Ref PrivateSubnet2Id]]
      MinSize: !Ref MinScalingNodes
      MaxSize: !Ref MaxScalingNodes
      Cooldown: '300'
      DesiredCapacity: !Ref MinScalingNodes
      HealthCheckType: EC2
      HealthCheckGracePeriod: 1800
      Tags:
        - Key: Name
          Value: !Ref DeploymentTag
          PropagateAtLaunch: true
        - Key: XrayVersion
          Value: !Ref XrayVersion
          PropagateAtLaunch: true
      TerminationPolicies:
        - OldestInstance
        - Default
    CreationPolicy:
      ResourceSignal:
        Count: !Ref MinScalingNodes
        Timeout: PT60M
  XrayLaunchConfiguration:
    Type: AWS::AutoScaling::LaunchConfiguration
    Metadata:
      AWS::CloudFormation::Authentication:
        S3AccessCreds:
          type: S3
          roleName:
            - !Ref XrayHostRole
          buckets:
            - !Ref QsS3BucketName
      AWS::CloudFormation::Init:
        configSets:
          xray_ami_setup:
            - "config-cloudwatch"
            - "config-ansible-xray-ami"
          xray_install:
            - "config-cloudwatch"
            - "config-ansible-xray-ami"
            - "config-xray"
        config-cloudwatch:
          files:
            /root/cloudwatch.conf:
              content: |
                [general]
                state_file = /var/awslogs/state/agent-state

                [/var/log/messages]
                file = /var/log/messages
                log_group_name = /xray/instances/{instance_id}
                log_stream_name = /var/log/messages/
                datetime_format = %b %d %H:%M:%S

                [/var/log/xray-ami-setup.log]
                file = /var/log/messages
                log_group_name = /xray/instances/{instance_id}
                log_stream_name = /var/log/xray-ami-setup.log
                datetime_format = %b %d %H:%M:%S

                [/var/log/xray.log]
                file = /var/log/messages
                log_group_name = /xray/instances/{instance_id}
                log_stream_name = /var/log/xray.log
                datetime_format = %b %d %H:%M:%S
              mode: "0400"
        config-ansible-xray-ami:
          files:
            /root/.xray_ami/xray-ami-setup.yml:
              content: !Sub |
                  # Base install for Xray
                  - import_playbook: xray-ami.yml
                    vars:
                      ami_creation: false
                      db_type: postgresql
                      db_driver: org.postgresql.Driver
                      xray_version: ${XrayVersion}
                      xray_ha_enabled: false
              mode: "0400"
        config-xray:
          files:
            /root/.xray_ami/xray.yml:
              content: !Sub |
                  # Base install for Xray
                  - import_playbook: site-xray.yml
                    vars:
                      jfrog_url: ${JfrogInternalUrl}
                      use_custom_data_directory: true
                      custom_data_directory: "${UserDataDirectory}"
                      master_key: ${MasterKey}
                      join_key: ${MasterKey}
                      db_type: ${DatabaseType}
                      db_driver: ${DatabaseDriver}
                      db_url: postgres://${XrayDatabaseUrl}
                      db_user: ${XrayDatabaseUser}
                      db_password: ${XrayDatabasePassword}
                      xray_version: ${XrayVersion}
              mode: "0400"
            /root/.vault_pass.txt:
              content: !Sub |
                ${AnsibleVaultPass}
              mode: "0400"
    Properties:
      KeyName: !Ref KeyPairName
      IamInstanceProfile: !Ref XrayHostProfile
      ImageId: !FindInMap
        - AWSAMIRegionMap
        - !Ref AWS::Region
        - 'CentOS7HVM'
      SecurityGroups:
        - !Ref SecurityGroups
      InstanceType: !Ref XrayInstanceType
      UserData:
        Fn::Base64:
          !Sub |
            #!/bin/bash -x
            exec > >(tee /var/log/user-data.log|logger -t user-data -s 2>/dev/console) 2>&1

            #CFN Functions

            function cfn_fail

            {

            cfn-signal -e 1 --stack ${AWS::StackName} --region ${AWS::Region} --resource XrayScalingGroup

            exit 1

            }

            function cfn_success

            {

            cfn-signal -e 0 --stack ${AWS::StackName} --region ${AWS::Region} --resource XrayScalingGroup

            exit 0

            }

            S3URI=${QsS3Uri}

            yum install -y git
            yum install -y postgresql-server postgresql-devel

            echo $PATH

            PATH=/opt/aws/bin:$PATH

            echo $PATH
            echo \'[Cloning: Load QuickStart Common Utils]\'

            git clone https://github.com/aws-quickstart/quickstart-linux-utilities.git

            source /quickstart-linux-utilities/quickstart-cfn-tools.source

            echo \'[Loaded: Load QuickStart Common Utils]\'

            echo \'[Update Operating System]\'

            qs_update-os || qs_err

            qs_bootstrap_pip || qs_err

            qs_aws-cfn-bootstrap || qs_err

            pip install virtualenv &> /var/log/userdata.virtualenv_install.log || qs_err " virtualenv install failed "

            virtualenv --system-site-packages ~/venv &> /var/log/userdata.create_venv.log || qs_err " creating venv failed "

            source ~/venv/bin/activate &> /var/log/userdata.activate_venv.log || qs_err " activate venv failed "

            pip install awscli &> /var/log/userdata.awscli_install.log || qs_err " awscli install failed "

            pip install ansible &> /var/log/userdata.ansible_install.log || qs_err " ansible install failed "

            pip install selinux &> /var/log/userdata.selinux_install.log || qs_err " selinux install failed "

            setsebool httpd_can_network_connect 1 -P

            yum update --security -y &> /var/log/userdata.yum_security_update.log || qs_err " security install failed "

            mkdir ~/.xray_ami

            aws s3 --region ${AWS::Region} sync s3://${QsS3BucketName}/${QsS3KeyPrefix}cloudInstallerScripts/ ~/.xray_ami/

            source ~/venv/bin/activate &> /var/log/userdata.activate_venv.log || qs_err " activate venv failed "

            cfn-init -v --stack ${AWS::StackName} --resource XrayLaunchConfiguration --configsets xray_install --region ${AWS::Region} || cfn_fail

            # Setup CloudWatch Agent
            curl https://s3.amazonaws.com/aws-cloudwatch/downloads/latest/awslogs-agent-setup.py -O
            chmod +x ./awslogs-agent-setup.py
            ./awslogs-agent-setup.py -n -r ${AWS::Region} -c /root/cloudwatch.conf

            # Get instance id from AWS
            INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            # Attach the volume created by another CFT
            # the device name should become /dev/nvme1n1
            # See: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/nvme-ebs-volumes.html
            /var/awslogs/bin/aws ec2 attach-volume --volume-id ${Volume} --instance-id $INSTANCE_ID --device /dev/xvdf --region ${AWS::Region} || cfn_fail
            echo "Wait for volume to attach"
            sleep 30 # Give volume time to attach
            lsblk # debug

            # CentOS cloned virtual machines do not create a new machine id
            # https://www.thegeekdiary.com/centos-rhel-7-how-to-change-the-machine-id/
            rm -f /etc/machine-id
            systemd-machine-id-setup

            if "true" == "${XrayFirstNode}"
            then
                psql postgresql://${DatabaseUser}:${DatabasePassword}@${XrayMasterDatabaseUrl} -c "CREATE USER ${XrayDatabaseUser} WITH PASSWORD '${XrayDatabasePassword}'" &>> /var/log/userdata.xray_database.log;
                psql postgresql://${DatabaseUser}:${DatabasePassword}@${XrayMasterDatabaseUrl} -c "grant ${XrayDatabaseUser} to ${DatabaseUser}" &>> /var/log/userdata.xray_database.log;
                psql postgresql://${DatabaseUser}:${DatabasePassword}@${XrayMasterDatabaseUrl} -c "CREATE DATABASE xraydb WITH OWNER=${XrayDatabaseUser} ENCODING='UTF8'" &>> /var/log/userdata.xray_database.log;
                psql postgresql://${DatabaseUser}:${DatabasePassword}@${XrayMasterDatabaseUrl} -c "GRANT ALL PRIVILEGES ON DATABASE xraydb TO ${XrayDatabaseUser}" &>> /var/log/userdata.xray_database.log;
            fi

            ansible-galaxy collection install community.general ansible.posix

            ansible-playbook /root/.xray_ami/xray-ami-setup.yml 2>&1 | tee /var/log/xray-ami.log || cfn_fail
            ansible-playbook /root/.xray_ami/xray.yml 2>&1 | tee /var/log/xray.log || qs_err " ansible execution failed "

            $(qs_status) &> /var/log/qs_status.log
            cfn_success &> /var/log/cfn_success.log
            [ $(qs_status) == 0 ] && cfn_success || cfn_fail
